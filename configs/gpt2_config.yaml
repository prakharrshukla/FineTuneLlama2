# GPT-2 Fine-tuning Configuration
model_name: "gpt2"
output_dir: "./models/gpt2_fine_tuned"

# Training Parameters
training:
  batch_size: 4
  num_epochs: 3
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_length: 512
  gradient_accumulation_steps: 1
  
# Optimization
optimization:
  fp16: true
  gradient_checkpointing: true
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  
# LoRA Configuration (optional)
lora:
  enabled: false
  r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["q_proj", "v_proj", "k_proj", "o_proj"]
  
# Data
data:
  train_file: "./data/sample_data.json"
  validation_split: 0.1
  
# Hardware
hardware:
  device: "cuda"
  mixed_precision: "fp16"
  
# Logging
logging:
  log_level: "INFO"
  save_steps: 100
  eval_steps: 500
  logging_steps: 50
